{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gradio'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgradio\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgr\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01munsloth\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FastLanguageModel  \u001b[38;5;66;03m# For efficient fine-tuning\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gradio'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n",
    "from datasets import Dataset\n",
    "import gradio as gr\n",
    "from unsloth import FastLanguageModel  # For efficient fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Set up GPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- Step 1: Load and Process Datasets ---\n",
    "# Define a function to load and process datasets\n",
    "def load_and_process_dataset(dataset_path):\n",
    "    \"\"\"\n",
    "    Load a dataset from the given path and process it into a format suitable for fine-tuning.\n",
    "    Each dataset should have a `column_info.csv` file describing the columns and data types.\n",
    "    \"\"\"\n",
    "    # Check if column_info.csv exists\n",
    "    column_info_path = os.path.join(dataset_path, \"column_info.csv\")\n",
    "    if not os.path.exists(column_info_path):\n",
    "        print(f\"Skipping dataset {dataset_path} because column_info.csv is missing.\")\n",
    "        return [], []\n",
    "    \n",
    "    # Load column information\n",
    "    column_info = pd.read_csv(column_info_path)\n",
    "    \n",
    "    # Find the main data file (e.g., csv file that is not column_info.csv)\n",
    "    data_files = [f for f in os.listdir(dataset_path) if f.endswith(\".csv\") and f != \"column_info.csv\"]\n",
    "    if not data_files:\n",
    "        print(f\"No data file found in {dataset_path}.\")\n",
    "        return [], []\n",
    "    \n",
    "    # Load the main data file\n",
    "    data_file_path = os.path.join(dataset_path, data_files[0])\n",
    "    data = pd.read_csv(data_file_path)\n",
    "    \n",
    "    # Create prompt-output pairs for fine-tuning\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    for _, row in data.iterrows():\n",
    "        # Create a prompt describing the columns\n",
    "        prompt = f\"Generate a dataset with columns: {', '.join(column_info['Column_Name'])}.\"\n",
    "        \n",
    "        # Create the table data as a CSV string\n",
    "        table_data = \",\".join([str(row[col]) for col in column_info['Column_Name']])\n",
    "        outputs.append(table_data)\n",
    "        inputs.append(prompt)\n",
    "    \n",
    "    return inputs, outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: The following columns are missing in the main data file for LLM_data/50K_Songs_Dataset_-_Generated_by_AI: ['track_name', 'artist_name', 'duration_ms', 'key', 'mode', 'tempo'].\n",
      "Warning: The following columns are missing in the main data file for LLM_data/US_Election_Dataset_: ['current_votes', 'total_votes', 'percent'].\n",
      "Total prompt-output pairs: 302262\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "def load_and_process_dataset(dataset_path):\n",
    "    \"\"\"\n",
    "    Load a dataset from the given path and process it into a format suitable for fine-tuning.\n",
    "    Each dataset folder has a `column_info.csv` file describing the columns and data types.\n",
    "    The `column_info.csv` file has the following columns:\n",
    "    - Column Name: The name of the column.\n",
    "    - Column Description: A description of the column.\n",
    "    - Column Type: The data type of the column.\n",
    "    \"\"\"\n",
    "    # Check if column_info.csv exists\n",
    "    column_info_path = os.path.join(dataset_path, \"column_info.csv\")\n",
    "    if not os.path.exists(column_info_path):\n",
    "        print(f\"Skipping dataset {dataset_path} because column_info.csv is missing.\")\n",
    "        return [], []\n",
    "    \n",
    "    # Load column information\n",
    "    column_info = pd.read_csv(column_info_path)\n",
    "    \n",
    "    # Check if the required columns exist in column_info.csv\n",
    "    required_columns = [\"Column Name\", \"Column Description\", \"Column Type\"]\n",
    "    if not all(col in column_info.columns for col in required_columns):\n",
    "        print(f\"Skipping dataset {dataset_path} because column_info.csv is missing required columns: {required_columns}.\")\n",
    "        return [], []\n",
    "    \n",
    "    # Find the main data file (e.g., csv file that is not column_info.csv)\n",
    "    data_files = [f for f in os.listdir(dataset_path) if f.endswith(\".csv\") and f != \"column_info.csv\"]\n",
    "    if not data_files:\n",
    "        print(f\"No data file found in {dataset_path}.\")\n",
    "        return [], []\n",
    "    \n",
    "    # Load the main data file\n",
    "    data_file_path = os.path.join(dataset_path, data_files[0])\n",
    "    data = pd.read_csv(data_file_path)\n",
    "    \n",
    "    # Check if all columns in column_info exist in the main data file\n",
    "    missing_columns = [col for col in column_info[\"Column Name\"] if col not in data.columns]\n",
    "    if missing_columns:\n",
    "        print(f\"Warning: The following columns are missing in the main data file for {dataset_path}: {missing_columns}.\")\n",
    "        # Remove missing columns from column_info\n",
    "        column_info = column_info[~column_info[\"Column Name\"].isin(missing_columns)]\n",
    "    \n",
    "    # Create prompt-output pairs for fine-tuning\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    for _, row in data.iterrows():\n",
    "        # Create a prompt describing the columns\n",
    "        column_descriptions = [\n",
    "            f\"{col_name} ({col_type}): {col_desc}\"\n",
    "            for col_name, col_desc, col_type in zip(\n",
    "                column_info[\"Column Name\"],\n",
    "                column_info[\"Column Description\"],\n",
    "                column_info[\"Column Type\"],\n",
    "            )\n",
    "        ]\n",
    "        prompt = f\"Generate a dataset with the following columns:\\n\" + \"\\n\".join(column_descriptions)\n",
    "        \n",
    "        # Create the table data as a CSV string\n",
    "        table_data = \",\".join([str(row[col]) for col in column_info[\"Column Name\"]])\n",
    "        outputs.append(table_data)\n",
    "        inputs.append(prompt)\n",
    "    \n",
    "    return inputs, outputs\n",
    "\n",
    "# Load all datasets\n",
    "dataset_paths = [\n",
    "    \"LLM_data/50K_Songs_Dataset_-_Generated_by_AI\",\n",
    "    \"LLM_data/Bank_Transaction_Dataset_for_Fraud_Detection\",\n",
    "    \"LLM_data/Customer_Feedback_and_Satisfaction\",\n",
    "    \"LLM_data/Data_Science_Job\",\n",
    "    \"LLM_data/Gym_Members_Exercise_Dataset\",\n",
    "    \"LLM_data/IMDB_Movie_Dataset\",\n",
    "    \"LLM_data/Loan_Approval_Classification_Dataset\",\n",
    "    \"LLM_data/Mobile_Device_Usage_and_User_Behavior_Dataset\",\n",
    "    \"LLM_data/New_York_Airbnb_Open_Data\",\n",
    "    \"LLM_data/US_Election_Dataset_\",\n",
    "]\n",
    "\n",
    "all_inputs = []\n",
    "all_outputs = []\n",
    "for path in dataset_paths:\n",
    "    inputs, outputs = load_and_process_dataset(path)\n",
    "    all_inputs.extend(inputs)\n",
    "    all_outputs.extend(outputs)\n",
    "\n",
    "# Convert to Hugging Face Dataset\n",
    "dataset = Dataset.from_dict({\"input\": all_inputs, \"output\": all_outputs})\n",
    "\n",
    "print(f\"Total prompt-output pairs: {len(all_inputs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in spotify_songs_dataset.csv: ['song_id', 'song_title', 'artist', 'album', 'genre', 'release_date', 'duration', 'popularity', 'stream', 'language', 'explicit_content', 'label', 'composer', 'producer', 'collaboration']\n",
      "Columns in column_info.csv: ['track_name', 'artist_name', 'duration_ms', 'popularity', 'genre', 'key', 'mode', 'tempo']\n",
      "Warning: The following columns are missing in the main data file: ['track_name', 'artist_name', 'duration_ms', 'key', 'mode', 'tempo'].\n",
      "Columns in US_Election_dataset_v1.csv: ['Unnamed: 0', 'county', 'state', '2020 Democrat vote raw', '2020 Democrat vote %', '2020 Republican vote raw', '2020 Republican vote %', '2020 other vote raw', '2020 other vote %', 'Population with less than 9th grade education', 'Population with 9th to 12th grade education, no diploma', 'High School graduate and equivalent', 'Some College,No Degree', 'Associates Degree', 'Bachelors Degree', 'Graduate or professional degree', 'Gini Index', 'Median income (dollars)', 'Mean income (dollars)', 'Area in square Km', 'Density per square km', 'Total Population', 'Hispanic or Latino percentage', 'NH-White percentage', 'NH-Black percentage', 'NH-American Indian and Alaska Native percentage', 'NH-Asian percentage', 'NH-Native Hawaiian and Other Pacific Islander percentage', 'NH-Some Other Race percentage', 'NH-Two or More Races percentage', 'Percentage engaged in Management, business, science, and arts occupations', 'Percentage engaged in Service Occupations', 'Percentage engaged in Sales and Office', 'Percentage engaged in Resources and Construction', 'Percentage engaged in Transportation']\n",
      "Columns in column_info.csv: ['state', 'county', 'current_votes', 'total_votes', 'percent']\n",
      "Warning: The following columns are missing in the main data file: ['current_votes', 'total_votes', 'percent'].\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def debug_dataset(dataset_path):\n",
    "    \"\"\"\n",
    "    Debug a dataset by printing the column names in the main data file\n",
    "    and comparing them with the column names in column_info.csv.\n",
    "    \"\"\"\n",
    "    # Check if column_info.csv exists\n",
    "    column_info_path = os.path.join(dataset_path, \"column_info.csv\")\n",
    "    if not os.path.exists(column_info_path):\n",
    "        print(f\"Skipping dataset {dataset_path} because column_info.csv is missing.\")\n",
    "        return\n",
    "    \n",
    "    # Load column information\n",
    "    column_info = pd.read_csv(column_info_path)\n",
    "    \n",
    "    # Find the main data file (e.g., csv file that is not column_info.csv)\n",
    "    data_files = [f for f in os.listdir(dataset_path) if f.endswith(\".csv\") and f != \"column_info.csv\"]\n",
    "    if not data_files:\n",
    "        print(f\"No data file found in {dataset_path}.\")\n",
    "        return\n",
    "    \n",
    "    # Load the main data file\n",
    "    data_file_path = os.path.join(dataset_path, data_files[0])\n",
    "    data = pd.read_csv(data_file_path)\n",
    "    \n",
    "    # Print column names in the main data file\n",
    "    print(f\"Columns in {data_files[0]}: {data.columns.tolist()}\")\n",
    "    \n",
    "    # Print column names in column_info.csv\n",
    "    print(f\"Columns in column_info.csv: {column_info['Column Name'].tolist()}\")\n",
    "    \n",
    "    # Check for missing columns\n",
    "    missing_columns = [col for col in column_info[\"Column Name\"] if col not in data.columns]\n",
    "    if missing_columns:\n",
    "        print(f\"Warning: The following columns are missing in the main data file: {missing_columns}.\")\n",
    "\n",
    "# Debug the 50K_Songs_Dataset_-_Generated_by_AI folder\n",
    "debug_dataset(\"LLM_data/50K_Songs_Dataset_-_Generated_by_AI\")\n",
    "\n",
    "# Debug the US_Election_Dataset_ folder\n",
    "debug_dataset(\"LLM_data/US_Election_Dataset_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total prompt-output pairs: 302262\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "def load_and_process_dataset(dataset_path):\n",
    "    \"\"\"\n",
    "    Load a dataset from the given path and process it into a format suitable for fine-tuning.\n",
    "    If column_info.csv exists, use it to provide descriptions for the columns.\n",
    "    If column_info.csv is missing or a column is missing in it, generate synthetic descriptions.\n",
    "    \"\"\"\n",
    "    # Find the main data file (e.g., csv file that is not column_info.csv)\n",
    "    data_files = [f for f in os.listdir(dataset_path) if f.endswith(\".csv\") and f != \"column_info.csv\"]\n",
    "    if not data_files:\n",
    "        print(f\"No data file found in {dataset_path}.\")\n",
    "        return [], []\n",
    "    \n",
    "    # Load the main data file\n",
    "    data_file_path = os.path.join(dataset_path, data_files[0])\n",
    "    data = pd.read_csv(data_file_path)\n",
    "    \n",
    "    # Initialize column descriptions\n",
    "    column_descriptions = {}\n",
    "    \n",
    "    # Check if column_info.csv exists\n",
    "    column_info_path = os.path.join(dataset_path, \"column_info.csv\")\n",
    "    if os.path.exists(column_info_path):\n",
    "        # Load column information\n",
    "        column_info = pd.read_csv(column_info_path)\n",
    "        \n",
    "        # Check if the required columns exist in column_info.csv\n",
    "        if \"Column Name\" in column_info.columns and \"Column Description\" in column_info.columns:\n",
    "            # Map column names to their descriptions\n",
    "            column_descriptions = dict(zip(column_info[\"Column Name\"], column_info[\"Column Description\"]))\n",
    "    \n",
    "    # Create prompt-output pairs for fine-tuning\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    for _, row in data.iterrows():\n",
    "        # Create a prompt describing the columns\n",
    "        prompt_columns = []\n",
    "        for col in data.columns:\n",
    "            if col in column_descriptions:\n",
    "                # Use the description from column_info.csv\n",
    "                prompt_columns.append(f\"{col}: {column_descriptions[col]}\")\n",
    "            else:\n",
    "                # Generate a synthetic description\n",
    "                prompt_columns.append(f\"{col}: The {col} of the record.\")\n",
    "        \n",
    "        prompt = f\"Generate a dataset with the following columns:\\n\" + \"\\n\".join(prompt_columns)\n",
    "        \n",
    "        # Create the table data as a CSV string\n",
    "        table_data = \",\".join([str(row[col]) for col in data.columns])\n",
    "        outputs.append(table_data)\n",
    "        inputs.append(prompt)\n",
    "    \n",
    "    return inputs, outputs\n",
    "\n",
    "# Load all datasets\n",
    "dataset_paths = [\n",
    "    \"LLM_data/50K_Songs_Dataset_-_Generated_by_AI\",\n",
    "    \"LLM_data/Bank_Transaction_Dataset_for_Fraud_Detection\",\n",
    "    \"LLM_data/Customer_Feedback_and_Satisfaction\",\n",
    "    \"LLM_data/Data_Science_Job\",\n",
    "    \"LLM_data/Gym_Members_Exercise_Dataset\",\n",
    "    \"LLM_data/IMDB_Movie_Dataset\",\n",
    "    \"LLM_data/Loan_Approval_Classification_Dataset\",\n",
    "    \"LLM_data/Mobile_Device_Usage_and_User_Behavior_Dataset\",\n",
    "    \"LLM_data/New_York_Airbnb_Open_Data\",\n",
    "    \"LLM_data/US_Election_Dataset_\",\n",
    "]\n",
    "\n",
    "all_inputs = []\n",
    "all_outputs = []\n",
    "for path in dataset_paths:\n",
    "    inputs, outputs = load_and_process_dataset(path)\n",
    "    all_inputs.extend(inputs)\n",
    "    all_outputs.extend(outputs)\n",
    "\n",
    "# Convert to Hugging Face Dataset\n",
    "dataset = Dataset.from_dict({\"input\": all_inputs, \"output\": all_outputs})\n",
    "\n",
    "print(f\"Total prompt-output pairs: {len(all_inputs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import torch\n",
    "\n",
    "# Load the model with Unsloth\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"meta-llama/Llama-2-7b-hf\",  # Replace with your model\n",
    "    max_seq_length=2048,  # Adjust based on your dataset\n",
    "    dtype=torch.float16,  # Use mixed precision for GPU\n",
    "    load_in_4bit=True,  # Use 4-bit quantization for memory efficiency\n",
    "    token=\"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\",  # Authenticate with Hugging Face\n",
    ")\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"input\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Fine-tune the model with Unsloth\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./fine_tuned_model\",  # Directory to save the fine-tuned model\n",
    "    per_device_train_batch_size=4,  # Adjust based on GPU memory\n",
    "    num_train_epochs=3,  # Number of training epochs\n",
    "    save_steps=10_000,  # Save checkpoint every 10,000 steps\n",
    "    save_total_limit=2,  # Keep only the last 2 checkpoints\n",
    "    fp16=True,  # Enable mixed precision for GPU\n",
    "    logging_dir=\"./logs\",  # Directory for logs\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    ")\n",
    "\n",
    "# Start fine-tuning\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained(\"./fine_tuned_model\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_model\")\n",
    "\n",
    "print(\"Fine-tuning complete and model saved to './fine_tuned_model'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
